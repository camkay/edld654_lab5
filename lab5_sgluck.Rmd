---
title: "Lab 5"
subtitle: "Random Forests/Bagging"
author: "Stephanie Gluck"
date: "11/23/2020"
output:
  html_document: 
    theme: spacelab
    toc: yes
    toc_depth: 4
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE,
                      cache = TRUE)

library(tidyverse)
library(tidymodels)
library(baguette)
library(future)

library(vip)
library(rpart.plot)

theme_set(theme_minimal())
options(scipen=1, digits=2)
```



## Data

Read in the `train.csv` data.

* Because some of the models will take some time run, randomly sample 1% of the data (be sure to use `set.seed`).
* Remove the *classification* variable.

Read in the `fallmembershipreport_20192020.xlsx` data.

* Select `Attending School ID`, `School Name`, and all columns that represent the race/ethnicity percentages for the schools (there is example code in recent class slides).

Join the two data sets.

If you have accessed outside data to help increase the performance of your models for the final project (e.g., [NCES](https://nces.ed.gov/)), you can read in and join those data as well.

```{r data}
data <- rio::import("data/data.csv") 
data_sub <- data %>% 
  sample_frac(.01)
```

## Split and Resample

Split joined data from above into a training set and test set, stratified by the outcome `score`.

Use 10-fold CV to resample the training set, stratified by `score`.

```{r split}
set.seed(3000)

splits <- initial_split(data_sub, strata = "score")
train <- training(splits)
test <- testing(splits)
cv_splits <- vfold_cv(train, stata = "score")
```

## Preprocess

Create one `recipe` to prepare your data for CART, bagged tree, and random forest models.

This lab could potentially serve as a template for your **Premilinary Fit 2**, or your final model prediction for the **Final Project**, so consider applying what might be your best model formula and the necessary preprocessing steps.

```{r rec}
rec <- recipe(score ~ ., train) %>%  
    step_mutate(tst_dt = lubridate::mdy_hm(tst_dt)) %>%
    update_role(contains("id"), ncessch, sch_name, new_role = "id vars") %>%
    step_novel(all_nominal()) %>%
    step_unknown(all_nominal()) %>%
    step_nzv(all_predictors(), freq_cut = 99/1) %>%
    step_normalize(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_BoxCox(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_dummy(all_nominal(), -has_role("id vars"), one_hot = TRUE) %>%
    step_zv(all_predictors())

rec

temp <- prep(rec) 

tidy(temp)

#novel
tidy(temp, 2) %>% 
  print(n = Inf)

#unknown
tidy(temp, 3) %>% 
  print(n = Inf)

#nzv
tidy(temp, 4) %>% 
  print(n = Inf)
```

## Decision Tree

1. Create a `parsnip` CART model using`{rpart}` for the estimation, tuning the cost complexity and minimum $n$ for a terminal node.
```{r dt_mod}
dt_tune_mod <- decision_tree() %>% 
  set_mode("regression") %>% 
  set_engine("rpart") %>% 
  set_args(cost_complexity = tune(), 
           min_n = tune())
```

2. Create a `workflow` object that combines your `recipe` and your `parsnip` objects.
```{r dt_workflow}
dt_tune_workflow <- workflow() %>%
  add_recipe(rec) %>%
  add_model(dt_tune_mod)
```

3. Tune your model with `tune_grid`
* Use `grid = 10` to choose 10 grid points automatically
* In the `metrics` argument, please include `rmse`, `rsq`, and `huber_loss`
* Record the time it takes to run. You could use `{tictoc}`, or you could do something like:
```{r dt_tuning, echo=TRUE}
start_rf <- Sys.time()

dt_tune_fit <- tune_grid(
  dt_tune_workflow,
  cv_splits,
  grid = 10,
  metrics = yardstick::metric_set(rmse, rsq, huber_loss),
  control = tune::control_resamples(verbose = FALSE, #turn off for knitted doc
                                    save_pred = TRUE))

end_rf <- Sys.time()
(dt_time <- end_rf - start_rf)
```


4. Show the best estimates for each of the three performance metrics and the tuning parameter values associated with each.
```{r dt_metrics}
dt_tune_fit %>% 
  collect_metrics(summarize = FALSE) 

dt_tune_fit %>%
  autoplot() +
  geom_line()

show_best(dt_tune_fit, metric = "rmse", n = 1) 
show_best(dt_tune_fit, metric = "rsq", n = 1) 
show_best(dt_tune_fit, metric = "huber_loss", n = 1) 

```

## Bagged Tree

1. Create a `parsnip` bagged tree model using`{baguette}` 
* specify 10 bootstrap resamples (only to keep run-time down), and 
* tune on `cost_complexity` and `min_n`

```{r bt_mod}
bt_mod <- bag_tree() %>% 
  set_mode("regression") %>% 
  set_engine("rpart", times = 10) %>% # 10 bootstrap re-samples
  set_args(cost_complexity = tune(), 
           min_n = tune())
```

2. Create a `workflow` object that combines your `recipe` and your bagged tree model specification.
```{r bt_workflow}
bt_tune_workflow <- workflow() %>%
  add_recipe(rec) %>%
  add_model(bt_mod)
```

3. Tune your model with `tune_grid`
* Use `grid = 10` to choose 10 grid points automatically
* In the `metrics` argument, please include `rmse`, `rsq`, and `huber_loss`
* In the `control` argument, please include `extract = function(x) extract_model(x)` to extract the model from each fit
* `{baguette}` is optimized to run in parallel with the `{future}` package. Consider using `{future}` to speed up processing time (see the class slides)
* Record the time it takes to run

#### **Question: Before you run the code, how many trees will this function execute?**

1,000 trees (10 folds * 10 grids * 10 models)

```{r bt_tuning}
plan(multisession)

start_rf <- Sys.time()

bt_tune_fit <- tune_grid(
  bt_tune_workflow, 
  cv_splits,
  grid = 10,
  metrics = yardstick::metric_set(rmse, rsq, huber_loss),
  control = control_resamples(verbose   = FALSE, #turn off for knitted doc
                              save_pred = TRUE,
                              extract = function(x) extract_model(x)))

end_rf <- Sys.time()
(bt_time <- end_rf - start_rf)

plan(sequential)
```

4. Show the single best estimates for each of the three performance metrics and the tuning parameter values associated with each.

```{r bt_metrics}
bt_tune_fit %>% 
  collect_metrics(summarize = FALSE) 

bt_tune_fit %>%
  autoplot() +
  geom_line()

show_best(bt_tune_fit, metric = "rmse", n = 1)
show_best(bt_tune_fit, metric = "rsq", n = 1) 
show_best(bt_tune_fit, metric = "huber_loss", n = 1) 
```

5. Run the `bag_roots` function below. Apply this function to the extracted bagged tree models from the previous step. This will output the feature at the root node for each of the decision trees fit. 

```{r, echo=TRUE}

bag_roots <- function(x){
  x %>% 
  select(.extracts) %>% 
  unnest(cols = c(.extracts)) %>% 
  mutate(models = map(.extracts,
                  ~.x$model_df)) %>% 
  select(-.extracts) %>% 
  unnest(cols = c(models)) %>% 
  mutate(root = map_chr(model,
                     ~as.character(.x$fit$frame[1, 1]))) %>%
  select(root)  
}

bt_roots <- bag_roots(bt_tune_fit)
```

6. Produce a plot of the frequency of features at the root node of the trees in your bagged model.


```{r bt_plot}
bt_roots %>% 
  group_by(root) %>% 
  count() %>% 
  ggplot(aes(n, reorder(root, n))) +
  geom_col(fill = "darkorange1", 
           alpha = .80) +
  labs(y = "Feature at Root Node",
       x = "Frequency (n)",
       title = "Tuned Bagged Models") + 
  scale_x_continuous(expand = c(0, 0))
```

## Random Forest

1. Create a `parsnip` random forest model using `{ranger}`
* use the `importance = "permutation"` argument to run variable importance
* specify 1,000 trees, but keep the other default tuning parameters

```{r rf_mod}
cores <- parallel::detectCores()

rf_tune_mod <- rand_forest() %>% 
  set_engine("ranger",
             num.threads = cores,
             importance = "permutation",
             verbose = TRUE) %>% 
  set_mode("regression") %>% 
  set_args(mtry = tune(),
           trees = 1000,
           min_n = tune())
```

2. Create a `workflow` object that combines your `recipe` and your random forest model specification.
```{r rf_workflow}
rf_tune_workflow <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(rf_tune_mod)
```

3. Fit your model 
* In the `metrics` argument, please include `rmse`, `rsq`, and `huber_loss`
* In the `control` argument, please include `extract = function(x) x` to extract the workflow from each fit
* Record the time it takes to run

```{r rf_tuning}
start_rf <- Sys.time()

rf_tune_fit <- tune_grid(
  rf_tune_workflow,
  cv_splits,
  metrics = yardstick::metric_set(rmse, rsq, huber_loss),
  control = control_resamples(verbose = FALSE, #turn off for knitted doc
                              save_pred = TRUE,
                              extract = function(x) x)
)

end_rf <- Sys.time()
(rf_time <- end_rf - start_rf)

```

4. Show the single best estimates for each of the three performance metrics.

```{r rf_metrics}

rf_tune_fit %>% 
  collect_metrics(summarize = FALSE) 

rf_tune_fit %>%
  autoplot() +
  geom_line()

show_best(rf_tune_fit, metric = "rmse", n = 1)
show_best(rf_tune_fit, metric = "rsq", n = 1) 
show_best(rf_tune_fit, metric = "huber_loss", n = 1) 

```

5. Run the two functions in the code chunk below. Then apply the `rf_roots` function to the results of your random forest model to output the feature at the root node for each of the decision trees fit in your random forest model. 

```{r, echo=TRUE}

rf_tree_roots <- function(x){
  map_chr(1:1000, 
           ~ranger::treeInfo(x, tree = .)[1, "splitvarName"])
}

rf_roots <- function(x){
  x %>% 
  select(.extracts) %>% 
  unnest(cols = c(.extracts)) %>% 
  mutate(fit = map(.extracts,
                   ~.x$fit$fit$fit),
         oob_rmse = map_dbl(fit,
                         ~sqrt(.x$prediction.error)),
         roots = map(fit, 
                        ~rf_tree_roots(.))
         ) %>% 
  select(roots) %>% 
  unnest(cols = c(roots))
}

rf_roots_tbl <- rf_roots(rf_tune_fit)
```

6. Produce a plot of the frequency of features at the root node of the trees in your bagged model.

```{r rf_plot}
rf_roots_tbl %>% 
  group_by(roots) %>% 
  count() %>% 
  filter(n > 300) %>% # to get only the most common count
  ggplot(aes(n, reorder(roots, n))) +
  geom_col(fill = "darkorange1", 
           alpha = .8) +
  labs(y = "Feature at Root Node",
       x = "Frequency (n)",
       title = "Tuned Random Forest Models")
```

7. Please explain why the bagged tree root node figure and the random forest root node figure are different.

**Bagged tree** generally split at variables with the strongest prediction to the outcome variable which results in very similar initial split at the top of the tree. When strong predictors are present, the strong predictors will drive the initial split which decreases the variability of subsequent splits and make tress from different resamples highly correlated to one another.  

**Random forest** decreases the correlation between different tree resamples by restricting splits to a random subset of the original predictors (mtry) which increases the variability of each tree leading to a more precise aggregated prediction.


8. Apply the `fit` function to your random forest `workflow` object and your **full** training data.
In class we talked about the idea that bagged tree and random forest models use resampling, and one *could* use the OOB prediction error provided by the models to estimate model performance.

* Record the time it takes to run
* Extract the oob prediction error from your fitted object. If you print your fitted object, you will see a value for *OOB prediction error (MSE)*. You can take the `sqrt()` of this value to get the *rmse*. Or you can extract it by running: `sqrt(fit-object-name-here$fit$fit$fit$prediction.error)`.

```{r oob_workflow}
#model used in the workflow
# rf_tune_mod <- rand_forest() %>% 
#   set_engine("ranger",
#              num.threads = cores,
#              importance = "permutation",
#              verbose = TRUE) %>% 
#   set_mode("regression") %>% 
#   set_args(mtry = tune(),
#            trees = 1000,
#            min_n = tune())

start_rf <- Sys.time()

set.seed(3000)

rf_oob_tune_fit <- fit(rf_tune_workflow,
                       data = data) #full training data

end_rf <- Sys.time()
end_rf - start_rf

#Received the warnings
# Warning messages:
# 1: tune columns were requested but there were 97 predictors in the data. 97 will be used. 
# 2: tune samples were requested but there were 189567 rows in the data. 189567 will be used. 

rf_oob_tune_fit

sqrt(rf_oob_tune_fit$fit$fit$fit$prediction.error)
#RMSE = 115.83, much higher!
```

We supplied a workflow object with tuning to our oob prediction. What if we removed the tuning parameters and used system default?

```{r oob_default}

rf_default_mod <- rand_forest() %>% 
   set_mode("regression") %>%
   set_engine("ranger",
              num.threads = cores, 
              importance = "permutation", 
              verbose = FALSE)

rf_default_workflow <- 
  workflow() %>% 
  add_model(rf_default_mod) %>% 
  add_recipe(rec)

#had to subset the data. The full data took so long to run, I stopped at 20 min. 

oob_data_sub <- data %>% 
  sample_frac(.1)

start_rf <- Sys.time()

set.seed(3000)

rf_oob_default_fit <- fit(rf_default_workflow,
                       data = oob_data_sub)

end_rf <- Sys.time()
(rfoob_time <- end_rf - start_rf)

rf_oob_default_fit

sqrt(rf_oob_default_fit$fit$fit$fit$prediction.error)
```

* How does OOB *rmse* here compare to the mean *rmse* estimate from your 10-fold CV random forest? How might 10-fold CV influence bias-variance?

The initial OOB RMSE of `r sqrt(rf_oob_tune_fit$fit$fit$fit$prediction.error)` I obtained with the full training data was much higher than the RMSE from my random forest 10-fold CV (RMSE = `r show_best(rf_tune_fit, metric = "rmse", n = 1)$mean`). I also received some error messages (noted on code chunk above) while running my initial OOB prediction. I later tried to remove the tuning parameters from my workflow and supplied a default random forest parameter for the OOB prediction. The full dataset took too long to run (I stopped at 20 min) and instead I ran the OOB estimate with a 10% subset of the full data. The default parameter OOB estimate produced an RMSE of `r sqrt(rf_oob_default_fit$fit$fit$fit$prediction.error)` which was slightly closer to the 10-fold CV RMSE but still not as low as the 10-fold CV RMSE. 

10-fold CV should help achieve a balance between bias-variance as the model fitting process repeats over 10 folds and the testing data in each fold is used in one and only one testing sample.


## Compare Performance 

Consider the four models you fit: (a) decision tree, (b) bagged tree, (c) random forest fit on resamples, and (d) random forest fit on the training data. Which model would you use for your final fit? Please consider the performance metrics as well as the run time, and briefly explain your decision. 

For the final model, a random forest model appears to provide the best RMSE compared to decision tree and bagged tree. While the RMSE was the lowest in the random forest model on the resample data with tuning (RMSE = `r show_best(rf_tune_fit, metric = "rmse", n = 1)$mean`) , the model took a long time to run (`r rf_time` min, using only 1% subset of the data too). I had some trouble running the random forest with the full training data (I had to use a 10% subset). Even with a 10% subset, the model ran much quicker (`r rfoob_time` sec) compared to using resamples and produced a similar RMSE of `r sqrt(rf_oob_default_fit$fit$fit$fit$prediction.error)`. Because of the similar RMSE but large decrease in model fitting time, I would chose the random forest model with the training data. However, given that I was still only sing a subset of the data, I would need to explore the code in my training data random forest model more closely. 

RMSE for

(a) decision tree = `r show_best(dt_tune_fit, metric = "rmse", n = 1)$mean`, time = `r dt_time` sec

(b) bagged tree = `r show_best(bt_tune_fit, metric = "rmse", n = 1)$mean`, time = `r bt_time` min

(c) random forest fit on resamples = `r show_best(rf_tune_fit, metric = "rmse", n = 1)$mean`, time = `r rf_time` min

(d) random forest fit on the training data = `r sqrt(rf_oob_default_fit$fit$fit$fit$prediction.error)`, time = `r rfoob_time` sec




---
title: "Lab 5"
subtitle: "Random Forests/Bagging"
author: "Ashley L. Miller"
date: "11/21/2020"
output:
  html_document:
    theme: spacelab
    toc: yes
    toc_depth: 4
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE,
                      cache = TRUE)

library(tidyverse)
library(tidymodels)
library(baguette)
library(future)
library(vip)
library(rpart.plot)
library(rio)
library(here)
library(magrittr)

update_geom_defaults('path', list(color = "#E4AACC"))
update_geom_defaults('point', list(color = "#880E4F", size = 1.75)) #was grey60
theme_set(theme_bw())
```

## Data

Read in the `train.csv` data.

* Because some of the models will take some time run, randomly sample 1% of the data (be sure to use `set.seed`).
* Remove the *classification* variable.

Read in the `fallmembershipreport_20192020.xlsx` data.

* Select `Attending School ID`, `School Name`, and all columns that represent the race/ethnicity percentages for the schools (there is example code in recent class slides).

Join the two data sets.

If you have accessed outside data to help increase the performance of your models for the final project (e.g., [NCES](https://nces.ed.gov/)), you can read in and join those data as well.

```{r data}
#Import merged data that we saved out already
data <- import(here::here("data", "data.csv"), setclass = "tbl_df") %>%
  janitor::clean_names() %>%
  sample_frac(.01)

head(data)
```

## Split and Resample

Split joined data from above into a training set and test set, stratified by the outcome `score`.

Use 10-fold CV to resample the training set, stratified by `score`.

```{r split_resample}
set.seed(3000)
data_split <- initial_split(data, strata = "score")
data_train <- training(data_split)
data_test  <- testing(data_split)
data_train_cv <- vfold_cv(data_train, strata = "score")
```

## Preprocess

Create one `recipe` to prepare your data for CART, bagged tree, and random forest models.

This lab could potentially serve as a template for your **Premilinary Fit 2**, or your final model prediction for the **Final Project**, so consider applying what might be your best model formula and the necessary preprocessing steps.

```{r rec}
rec <- recipe(score ~ ., data_train) %>%  
    step_mutate(tst_dt = lubridate::mdy_hm(tst_dt)) %>%
    update_role(contains("id"), ncessch, sch_name, new_role = "id vars") %>%
    step_novel(all_nominal()) %>%
    step_unknown(all_nominal()) %>%
    step_nzv(all_predictors(), freq_cut = 99/1) %>%
    step_normalize(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_BoxCox(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_dummy(all_nominal(), -has_role("id vars"), one_hot = TRUE) %>%
    step_zv(all_predictors())
```

## Decision Tree

1. Create a `parsnip` CART model using`{rpart}` for the estimation, tuning the cost complexity and minimum $n$ for a terminal node.

```{r dt_mod}
dt_tune_mod <- decision_tree() %>% 
  set_mode("regression") %>% 
  set_engine("rpart") %>% 
  set_args(cost_complexity = tune(), 
           min_n = tune())
```

2. Create a `workflow` object that combines your `recipe` and your `parsnip` objects.

```{r dt_workflow}
dt_tune_workflow <- workflow() %>%
  add_recipe(rec) %>%
  add_model(dt_tune_mod)
```

3. Tune your model with `tune_grid`
* Use `grid = 10` to choose 10 grid points automatically
* In the `metrics` argument, please include `rmse`, `rsq`, and `huber_loss`
* Record the time it takes to run. You could use `{tictoc}`, or you could do something like:

```{r dt_tune, echo=TRUE}
start_rf <- Sys.time()

dt_tune_fit <- tune_grid(
  dt_tune_workflow,
  data_train_cv,
  grid = 10,
  metrics = yardstick::metric_set(rmse, rsq, huber_loss),
  control = tune::control_resamples(verbose = FALSE, #turn off for knitted doc
                                    save_pred = TRUE))

end_rf <- Sys.time()
end_rf - start_rf #1.45 minutes
```

4. Show the best estimates for each of the three performance metrics and the tuning parameter values associated with each.

```{r dt_metrics}
dt_tune_fit %>%
  autoplot() +
  geom_line()

show_best(dt_tune_fit, metric = "rmse", n = 1) # rmse = 95.4
show_best(dt_tune_fit, metric = "rsq", n = 1) #rsq = .302
show_best(dt_tune_fit, metric = "huber_loss", n = 1) #huber loss = 74.6
```

## Bagged Tree

1. Create a `parsnip` bagged tree model using`{baguette}` 
* specify 10 bootstrap resamples (only to keep run-time down), and 
* tune on `cost_complexity` and `min_n`

```{r bt_mod}
bag_tune_mod <- bag_tree() %>% 
  set_mode("regression") %>% 
  set_engine("rpart", times = 10) %>% # 10 bootstrap re-samples
  set_args(cost_complexity = tune(), 
           min_n = tune())
```

2. Create a `workflow` object that combines your `recipe` and your bagged tree model specification.

```{r bt_workflow}
bag_tune_workflow <- workflow() %>%
  add_recipe(rec) %>%
  add_model(bag_tune_mod)
```

3. Tune your model with `tune_grid`
* Use `grid = 10` to choose 10 grid points automatically
* In the `metrics` argument, please include `rmse`, `rsq`, and `huber_loss`
* In the `control` argument, please include `extract = function(x) extract_model(x)` to extract the model from each fit
* `{baguette}` is optimized to run in parallel with the `{future}` package. Consider using `{future}` to speed up processing time (see the class slides)
* Record the time it takes to run

#### **Question: Before you run the code, how many trees will this function execute?**

1,000 trees (10 folds * 10 grids * 10 models)

```{r bt_tune}
library(future)
plan(multisession)

library(tictoc)
tic()

bag_tune_fit <- tune_grid(
  bag_tune_workflow,
  data_train_cv,
  grid = 10,
  metrics = yardstick::metric_set(rmse, rsq, huber_loss),
  control = tune::control_resamples(verbose = FALSE, #turn off for knitted doc
                                    save_pred = TRUE,
                                    extract = function(x) extract_model(x)))

toc() # 254.446 sec elapsed

plan(sequential)
```

4. Show the single best estimates for each of the three performance metrics and the tuning parameter values associated with each.

```{r bt_metrics}
bag_tune_fit %>% 
  autoplot() +
  geom_line()

show_best(bag_tune_fit, "rmse", n = 1) #rmse = 88.7
show_best(bag_tune_fit, "rsq", n = 1) #rsq = 0.379
show_best(bag_tune_fit, "huber_loss", n = 1) #huber loss = 69.4
```

5. Run the `bag_roots` function below. Apply this function to the extracted bagged tree models from the previous step. This will output the feature at the root node for each of the decision trees fit. 

```{r bag_roots, echo=TRUE}
bag_roots <- function(x){
  x %>% 
  select(.extracts) %>% 
  unnest(cols = c(.extracts)) %>% 
  mutate(models = map(.extracts,
                  ~.x$model_df)) %>% 
  select(-.extracts) %>% 
  unnest(cols = c(models)) %>% 
  mutate(root = map_chr(model,
                     ~as.character(.x$fit$frame[1, 1]))) %>%
  select(root)  
}

(roots_bag_tune <- bag_roots(bag_tune_fit))
```

6. Produce a plot of the frequency of features at the root node of the trees in your bagged model.

```{r bag_roots_plot}
roots_bag_tune %>% 
  group_by(root) %>% 
  count() %>% 
  ggplot(aes(n, reorder(root, n))) +
  geom_col(fill = "#E4AACC", 
           alpha = .70) +
  labs(y = "Feature at Root Node",
       x = "Frequency (n)",
       title = "Tuned Bagged Models") + 
  scale_x_continuous(expand = c(0, 0))
```

## Random Forest

1. Create a `parsnip` random forest model using `{ranger}`
* use the `importance = "permutation"` argument to run variable importance
* specify 1,000 trees, but keep the other default tuning parameters

```{r rf_mod}
(cores <- parallel::detectCores())

rf_tune_mod <- rand_forest() %>% 
  set_mode("regression") %>%
  set_engine("ranger",
             num.threads = cores, 
             importance = "permutation", 
             verbose = FALSE) %>% #turn off for knitted doc
  set_args(mtry = tune(),
           trees = 1000,
           min_n = tune())

#translate(rf_tune_mod)
```

2. Create a `workflow` object that combines your `recipe` and your random forest model specification.
```{r rf_workflow}
rf_tune_workflow <- 
  workflow() %>% 
  add_model(rf_tune_mod) %>% 
  add_recipe(rec)
```

3. Fit your model 
* In the `metrics` argument, please include `rmse`, `rsq`, and `huber_loss`
* In the `control` argument, please include `extract = function(x) x` to extract the workflow from each fit
* Record the time it takes to run

```{r rf_tune}
tictoc::tic()

set.seed(3000)

rf_tune_fit <- tune_grid(
  rf_tune_workflow,
  data_train_cv,
  metrics = yardstick::metric_set(rmse, rsq, huber_loss),
  control = control_resamples(verbose = FALSE, #turn off for knitted doc
                              save_pred = TRUE,
                              extract = function(x) x))

tictoc::toc() #516.274 sec (aka about 9 minutes)
```

4. Show the single best estimates for each of the three performance metrics.

```{r rf_metrics}
show_best(rf_tune_fit, "rmse", n = 1) #rmse = 86.7, mtry = 47, min_n = 30
show_best(rf_tune_fit, "rsq", n = 1) #rsq = .413, mtry = 16, min_n = 40
show_best(rf_tune_fit, "huber_loss", n = 1) #huber loss = 67.8 mtry = 47, min_n = 30

rf_tune_fit %>% 
  autoplot() +
  geom_line()
```

Finally, lets include the finalized rf model adopting the best tuning metrics:

```{r rf_final_fit}
rf_best <- select_best(rf_tune_fit, metric = "rmse")

rf_wf_final <- finalize_workflow(
  rf_tune_workflow,
  rf_best)

rf_wf_final

tictoc::tic()

set.seed(3000)

rf_res_final <- last_fit(rf_wf_final,
                         split = data_split)

tictoc::toc() # 7 seconds

collect_metrics(rf_res_final)
```

5. Run the two functions in the code chunk below. Then apply the `rf_roots` function to the results of your random forest model to output the feature at the root node for each of the decision trees fit in your random forest model. 

```{r rf_roots, echo=TRUE}
rf_tree_roots <- function(x){
  map_chr(1:1000, 
           ~ranger::treeInfo(x, tree = .)[1, "splitvarName"])
}

rf_roots <- function(x){
  x %>% 
  select(.extracts) %>% 
  unnest(cols = c(.extracts)) %>% 
  mutate(fit = map(.extracts,
                   ~.x$fit$fit$fit),
         oob_rmse = map_dbl(fit,
                         ~sqrt(.x$prediction.error)),
         roots = map(fit, 
                        ~rf_tree_roots(.))
         ) %>% 
  select(roots) %>% 
  unnest(cols = c(roots))
}

(roots_rf_tune <- rf_roots(rf_tune_fit))
```

6. Produce a plot of the frequency of features at the root node of the trees in your bagged model.

```{r rf_root_plot}
roots_rf_tune %>% 
  group_by(roots) %>% 
  count() %>% 
  filter(n > 400) %>% # I only want the top 15 or so 
  ggplot(aes(n, reorder(roots, n))) +
  geom_col(fill = "#880E4F", 
           alpha = .60) +
  labs(y = "Feature at Root Node",
       x = "Frequency (n)",
       title = "Tuned Random Forest Models") + 
  scale_x_continuous(expand = c(0, 0),
                     breaks = c(0, 5000, 10000, 15000, 20000, 25000 ),
                     labels = c("0", "5,000", "10,000", "15,000", 
                                "20,000", "25,000"))
```

7. Please explain why the bagged tree root node figure and the random forest root node figure are different.

*Bagged tree models* generally split at variables that are most relevant to predicting the outcome variable. For instance, most of the trees will use the strongest predictor in the top split; hence trees from different resamples generally have similar structure to each other (especially at the top of the tree). Conversely, *random forest models* aim to reduce underlying strong relations across trees by splitting at a randomly selected root variable (which is limited to a random subset of the original predictors). In reducing the _tree correlation_, each tree is more different, which tends to result in more accurate decisions. 

8. Apply the `fit` function to your random forest `workflow` object and your **full** training data.
In class we talked about the idea that bagged tree and random forest models use resampling, and one *could* use the OOB prediction error provided by the models to estimate model performance.

* Record the time it takes to run

```{r rf_oob_fit}
# model used in workflow:
# rf_tune_mod <- rand_forest() %>% 
#   set_mode("regression") %>%
#   set_engine("ranger",
#              num.threads = cores, 
#              importance = "permutation", 
#              verbose = TRUE) %>% 
#   set_args(mtry = tune(),
#            trees = 1000,
#            min_n = tune())

tictoc::tic()

set.seed(3000)

rf_oob_tune_fit <- fit(rf_tune_workflow,
                       data = data) #full training data.. not sure why we're using this

tictoc::toc() # 3.84 seconds

# I got the following warnings:
#Warning messages:
#1: tune columns were requested but there were 93 predictors in the data. 93 will be used. 
#2: tune samples were requested but there were 1896 rows in the data. 1896 will be used.
```

Maybe we can't have tuning parameters in this scenario?? 

```{r rf_default_oob_fit}
rf_default_mod <- rand_forest() %>% 
   set_mode("regression") %>%
   set_engine("ranger",
              num.threads = cores, 
              importance = "permutation", 
              verbose = FALSE) %>% #turn off for knitted doc
   set_args(trees = 1000) #removed tuning parameters

rf_default_workflow <- 
  workflow() %>% 
  add_model(rf_default_mod) %>% 
  add_recipe(rec)

tictoc::tic()

set.seed(3000)

rf_oob_default_fit <- fit(rf_default_workflow,
                       data = data)

tictoc::toc() # 8.555 seconds
```

* Extract the oob prediction error from your fitted object. If you print your fitted object, you will see a value for *OOB prediction error (MSE)*. You can take the `sqrt()` of this value to get the *rmse*. Or you can extract it by running: `sqrt(fit-object-name-here$fit$fit$fit$prediction.error)`.

```{r rf_oob_rmse}
sqrt(rf_oob_tune_fit$fit$fit$fit$prediction.error) #rmse = 113.14 YIKES
sqrt(rf_oob_default_fit$fit$fit$fit$prediction.error) #rmse = 87.71 MUCH better
```

* How does OOB *rmse* here compare to the mean *rmse* estimate from your 10-fold CV random forest? How might 10-fold CV influence bias-variance?

The OOB *rmse* here ranges from 87.71 to 113.14. The former *rmse* is obtained when using the workflow object where I deleted the tuning parameters for `mtry` and `min_n`, whereas the latter *rmse* is obtained when using the workflow object where I included the tuning parameters. Since the inclusion of these tuning parameters produced a series of troublesome warnings and ultimately resulted in a much larger *rmse*, I will hereafter ignore this model.

The OOB *rmse* (using the workflow object where I deleted the tuning parameters) is actually quite similar to--albeit higher than--the *rmse* from our 10-fold cross validation random forest model (*rmse* = 86.7). Given how well the data are predicted, 10-fold cross validation appears to produce models with relatively low bias (i.e., high performance). While a bias-variance trade-off would suggest that 10-fold cross validation may therefore have more variability, the variability wasn't too bad (i.e., results did not change drastically across samples--see `rf_res_final` above). As such, 10-fold cross validation seems to do a fine job resolving any trade-off between bias-variance. This makes sense considering the model fitting process repeats over 10 "folds", and each fold contains a sample in which none of the observations are repeated.

## Compare Performance 

Consider the four models you fit: (a) decision tree, (b) bagged tree, (c) random forest fit on resamples, and (d) random forest fit on the training data. Which model would you use for your final fit? Please consider the performance metrics as well as the run time, and briefly explain your decision. 

For my final fit, I would chose one of the random forest models. Both of these models were superior to the decision tree and bagged tree models in terms of performance metrics (*rmse*'s = 95.4 and 88.7, respectively). The random forest fit on resamples (with tuning) yielded the overall best *rmse*, but it took 9 minutes to run... and this was when we used only 1% of the data!! The random forest fit on the training data (with no tuning) yielded a smaller *rmse*, but it only took 9 seconds to run. Considering the *rmse* difference between the two random forest models came down to a mere one point difference, I'd probably use the random forest fit on training data in this case. 


